"""
OpenAIæµå¼å“åº”å¤„ç†æ¨¡å—

æä¾›OpenAIæµå¼ChatCompletionå“åº”çš„ç´¯åŠ å’Œè½¬æ¢åŠŸèƒ½ã€‚
å°†æµå¼çš„ChatCompletionChunkç´¯ç§¯å¹¶è½¬æ¢ä¸ºå®Œæ•´çš„ChatCompletionå¯¹è±¡ã€‚

ä¸»è¦åŠŸèƒ½:
- StreamAccumulator: ç´¯åŠ æµå¼chunkå¹¶è½¬æ¢ä¸ºå®Œæ•´å“åº”
- æ”¯æŒæ¨ç†å†…å®¹ï¼ˆreasoning_contentï¼‰å¤„ç†
- å®æ—¶æ˜¾ç¤ºå’Œå›è°ƒæ”¯æŒ
"""

from dataclasses import dataclass, field
from typing import Optional, Callable, List
from openai.types.chat import ChatCompletion, ChatCompletionChunk, ChatCompletionMessage
from openai.types.chat.chat_completion import Choice

from .common_utils import RealTimeDisplayHandler

__all__ = ['StreamAccumulator']


def _chatcompletionchunk_delta_has_reasoning(delta):
    """
    æ£€æŸ¥deltaæ˜¯å¦åŒ…å«æ¨ç†å†…å®¹

    DeepSeekç­‰æ¨¡å‹é€šè¿‡reasoning_contentå±æ€§æä¾›æ¨ç†è¿‡ç¨‹ã€‚

    å‚æ•°:
        delta: ChatCompletionChunkçš„deltaå¯¹è±¡

    è¿”å›:
        bool: æ˜¯å¦åŒ…å«æ¨ç†å†…å®¹
    """
    return hasattr(delta, 'reasoning_content') and delta.reasoning_content

def chunks_to_complete_response(chunks: List[ChatCompletionChunk]) -> ChatCompletion:
    """
    å°†æµå¼chunkåˆ—è¡¨è½¬æ¢ä¸ºå®Œæ•´çš„ChatCompletionå“åº”

    éå†æ‰€æœ‰chunkï¼Œç´¯ç§¯å†…å®¹ã€æ¨ç†å†…å®¹å’Œå…ƒæ•°æ®ï¼Œ
    æœ€ç»ˆæ„é€ å®Œæ•´çš„ChatCompletionå¯¹è±¡ã€‚

    å‚æ•°:
        chunks: ChatCompletionChunkå¯¹è±¡åˆ—è¡¨

    è¿”å›:
        å®Œæ•´çš„ChatCompletionå¯¹è±¡

    å¤„ç†é€»è¾‘:
        1. ä»é¦–ä¸ªchunkæå–åŸºç¡€ä¿¡æ¯ï¼ˆidã€modelç­‰ï¼‰
        2. æŒ‰choice.indexåˆ†ç»„ç´¯ç§¯å†…å®¹
        3. åˆå¹¶æ‰€æœ‰delta.contentå’Œdelta.reasoning_content
        4. ä»æœ€åä¸€ä¸ªchunkè·å–usageä¿¡æ¯
    """
    # æ”¶é›†åŸºç¡€ä¿¡æ¯çš„å˜é‡
    id = None
    created = None
    model = None
    system_fingerprint = None

    # ç”¨äºæ”¶é›†æ‰€æœ‰choiceçš„ä¿¡æ¯ï¼ˆæŒ‰indexåˆ†ç»„ï¼‰
    choices_data = {}
    usage = None

    for chunk in chunks:
        # ä»ç¬¬ä¸€ä¸ªchunkè·å–åŸºç¡€ä¿¡æ¯
        if id is None:
            id = chunk.id
            created = chunk.created
            model = chunk.model
            system_fingerprint = chunk.system_fingerprint

        if chunk.choices:
            for choice in chunk.choices:
                index = choice.index
                delta = choice.delta

                # åˆå§‹åŒ–choiceæ•°æ®ç»“æ„
                if index not in choices_data:
                    choices_data[index] = {
                        'content_parts': [],           # æ™®é€šå†…å®¹ç‰‡æ®µ
                        'reasoning_content_parts': []  # æ¨ç†å†…å®¹ç‰‡æ®µ
                    }
                choice_data = choices_data[index]

                # ç´¯ç§¯å†…å®¹ç‰‡æ®µ
                if delta.content:
                    choice_data['content_parts'].append(delta.content)
                if _chatcompletionchunk_delta_has_reasoning(delta):
                    choice_data['reasoning_content_parts'].append(delta.reasoning_content)

                # æ”¶é›†å…ƒæ•°æ®ï¼ˆæ¯æ¬¡æ›´æ–°ï¼Œä¿ç•™æœ€æ–°å€¼ï¼‰
                choice_data['role'] = delta.role or choice_data.get('role')
                choice_data['function_call'] = delta.function_call or choice_data.get('function_call')
                choice_data['tool_calls'] = delta.tool_calls or choice_data.get('tool_calls')
                choice_data['finish_reason'] = choice.finish_reason or choice_data.get('finish_reason')

        # è·å–usageä¿¡æ¯ï¼ˆä»…åœ¨æœ€åä¸€ä¸ªchunkä¸­ï¼‰
        if chunk.usage:
            usage = chunk.usage

    # æ„å»ºæ‰€æœ‰choiceå¯¹è±¡
    choices = []
    for index in sorted(choices_data.keys()):
        choice_data = choices_data[index]

        # æ„å»ºæ¶ˆæ¯å¯¹è±¡
        message = dict(
            content=''.join(choice_data['content_parts']),        # åˆå¹¶æ‰€æœ‰å†…å®¹ç‰‡æ®µ
            role=choice_data.get('role') or 'assistant',
            function_call=choice_data.get('function_call'),
            tool_calls=choice_data.get('tool_calls')
        )

        # æ·»åŠ æ¨ç†å†…å®¹ï¼ˆå¦‚æœæœ‰ï¼‰
        if choice_data['reasoning_content_parts']:
            full_reasoning_content = ''.join(choice_data['reasoning_content_parts'])
            message['reasoning_content'] = full_reasoning_content

        message = ChatCompletionMessage(**message)

        # æ„å»ºChoiceå¯¹è±¡
        choice = Choice.model_construct(
            finish_reason=choice_data.get('finish_reason'),
            index=index,
            message=message,
            logprobs=None
        )
        choices.append(choice)

    # åˆ›å»ºå®Œæ•´çš„ChatCompletionå¯¹è±¡
    chat_completion = ChatCompletion(
        id=id,
        choices=choices,
        created=created,
        model=model,
        object='chat.completion',
        system_fingerprint=system_fingerprint,
        usage=usage
    )

    return chat_completion


@dataclass
class StreamAccumulator(RealTimeDisplayHandler):
    """
    OpenAIæµå¼å“åº”ç´¯åŠ å™¨

    ç´¯ç§¯æµå¼ChatCompletionChunkå¹¶æä¾›è½¬æ¢ä¸ºå®Œæ•´ChatCompletionçš„åŠŸèƒ½ã€‚
    ç»§æ‰¿RealTimeDisplayHandlerä»¥æ”¯æŒå®æ—¶æ˜¾ç¤ºã€‚

    å±æ€§:
        chunks: ç´¯ç§¯çš„ChatCompletionChunkåˆ—è¡¨

    æ–¹æ³•:
        add_chunk: æ·»åŠ æ–°çš„chunkå¹¶å¤„ç†å›è°ƒ/æ˜¾ç¤º
        to_complete_response: å°†ç´¯ç§¯çš„chunksè½¬æ¢ä¸ºå®Œæ•´å“åº”
    """

    chunks: List[ChatCompletionChunk] = field(default_factory=list)

    def add_chunk(
        self,
        chunk: ChatCompletionChunk,
        callback: Optional[Callable[[str, bool, int], None]] = None,
        realtime_display: bool = False,
        show_thinking: bool = False
    ):
        """
        æ·»åŠ æ–°çš„ chunk å¹¶å¤„ç†å›è°ƒå’Œå®æ—¶æ˜¾ç¤º

        ã€è°ƒç”¨é“¾ã€‘
        _handle_sync()
            â†“
        for chunk in stream:
            â†“
        add_chunk(chunk) [å½“å‰] Ã— N
            â†“
        to_complete_response()

        ã€å†…éƒ¨è°ƒç”¨ã€‘
        1. self.chunks.append(chunk)
           - ç´¯ç§¯ chunk åˆ°åˆ—è¡¨

        2. å¯¹æ‰€æœ‰ choice æ‰§è¡Œå›è°ƒï¼ˆå¦‚æœæä¾›ï¼‰:
           for choice in chunk.choices:
               â”œâ”€ callback(delta.reasoning_content, is_thinking=True, index)
               â””â”€ callback(delta.content, is_thinking=False, index)

        3. å®æ—¶æ˜¾ç¤ºç¬¬ä¸€ä¸ª choice:
           self._handle_realtime_display(text, is_thinking, show_thinking)
               â”œâ”€ é¦–æ¬¡æ€è€ƒ: æ‰“å° "ğŸ¤” æ€è€ƒè¿‡ç¨‹:" + åˆ†éš”ç¬¦
               â”œâ”€ é¦–æ¬¡å›ç­”: æ‰“å° "ğŸ’¡ å›ç­”:"
               â””â”€ print(text, end='', flush=True)

        ã€è¢«è°ƒç”¨ã€‘
        - BaseSDKChatBot._handle_sync() - åŒæ­¥æµå¼å¤„ç†
        - BaseSDKChatBot._handle_async() - å¼‚æ­¥æµå¼å¤„ç†
        - _handle_complete_response() - Google å®Œæ•´å“åº”å¤„ç†ï¼ˆå¤ç”¨ï¼‰

        ã€å‚æ•°ã€‘
        chunk: ChatCompletionChunk å¯¹è±¡
              - choices[0].delta.content: å¢é‡æ–‡æœ¬
              - choices[0].delta.reasoning_content: å¢é‡æ¨ç†ï¼ˆDeepSeekï¼‰
        callback: å›è°ƒå‡½æ•°ï¼Œç­¾å callback(content: str, is_thinking: bool, index: int)
        realtime_display: æ˜¯å¦å®æ—¶æ‰“å°åˆ°ç»ˆç«¯
        show_thinking: æ˜¯å¦æ˜¾ç¤ºæ€è€ƒå†…å®¹ï¼ˆä»…å½“ realtime_display=True æ—¶æœ‰æ•ˆï¼‰

        ã€è¿”å›ã€‘
        Noneï¼ˆå‰¯ä½œç”¨ï¼šä¿®æ”¹ self.chunksï¼Œæ‰§è¡Œå›è°ƒå’Œæ‰“å°ï¼‰

        ã€OpenAI ç‰¹ç‚¹ã€‘
        - Delta å¢é‡æ¨¡å¼ï¼šæ¯ä¸ª chunk åªåŒ…å«æ–°å¢å†…å®¹
        - æ¨ç†å†…å®¹é€šè¿‡ reasoning_content å±æ€§æš´éœ²ï¼ˆDeepSeekï¼‰
        - å¤š choice æ”¯æŒï¼šéå†æ‰€æœ‰ choice æ‰§è¡Œå›è°ƒ
        """
        self.chunks.append(chunk)
        if not chunk.choices:
            return

        # å¤„ç†æ‰€æœ‰choiceçš„å›è°ƒ
        for choice in chunk.choices:
            index = choice.index
            delta = choice.delta

            if callback:
                # å›è°ƒæ¨ç†å†…å®¹
                if _chatcompletionchunk_delta_has_reasoning(delta):
                    callback(delta.reasoning_content, True, index)
                # å›è°ƒæ™®é€šå†…å®¹
                if delta.content:
                    callback(delta.content, False, index)

        # å®æ—¶æ˜¾ç¤ºç¬¬ä¸€ä¸ªchoice
        if realtime_display:
            delta = chunk.choices[0].delta
            if _chatcompletionchunk_delta_has_reasoning(delta):
                self._handle_realtime_display(delta.reasoning_content, True, show_thinking)
            if delta.content:
                self._handle_realtime_display(delta.content, False, False)

    def to_complete_response(self) -> ChatCompletion:
        """
        å°†ç´¯ç§¯çš„ chunks è½¬æ¢ä¸ºå®Œæ•´çš„ ChatCompletion å¯¹è±¡

        ã€è°ƒç”¨é“¾ã€‘
        _handle_sync()
            â†“
        for chunk in stream:
            add_chunk(chunk) Ã— N
            â†“
        to_complete_response() [å½“å‰]
            â†“
        è¿”å› ChatCompletion

        ã€å†…éƒ¨è°ƒç”¨ã€‘
        chunks_to_complete_response(self.chunks)
            â†“
        å¤„ç†é€»è¾‘:
            1. ä»é¦–ä¸ª chunk æå–: id, created, model, system_fingerprint
            2. æŒ‰ choice.index åˆ†ç»„ç´¯ç§¯:
               â”œâ”€ content_parts.append(delta.content)
               â””â”€ reasoning_content_parts.append(delta.reasoning_content)
            3. åˆå¹¶å†…å®¹ç‰‡æ®µ:
               â”œâ”€ content = ''.join(content_parts)
               â””â”€ reasoning_content = ''.join(reasoning_content_parts)
            4. ä»æœ€å chunk æå–: usage
            5. æ„é€  ChatCompletion å¯¹è±¡

        ã€è¢«è°ƒç”¨ã€‘
        - BaseSDKChatBot._handle_sync() - æµå¼å¤„ç†å®Œæˆå
        - BaseSDKChatBot._handle_async() - å¼‚æ­¥æµå¼å¤„ç†å®Œæˆå

        ã€è¿”å›ã€‘
        ChatCompletion å¯¹è±¡:
            - id, created, model, system_fingerprint: ä»é¦–ä¸ª chunk
            - choices: åˆå¹¶æ‰€æœ‰ delta åçš„å®Œæ•´æ¶ˆæ¯
            - usage: ä»æœ€å chunkï¼ˆéœ€è¦ stream_options={'include_usage': True}ï¼‰

        ã€OpenAI ç‰¹ç‚¹ã€‘
        - Delta ç´¯ç§¯ï¼šå°†æ‰€æœ‰å¢é‡å†…å®¹æ‹¼æ¥æˆå®Œæ•´å­—ç¬¦ä¸²
        - æ”¯æŒå¤š choiceï¼šæŒ‰ index åˆ†ç»„å¤„ç†
        - usage ä¿¡æ¯ï¼šä»…åœ¨æœ€åä¸€ä¸ª chunk ä¸­å­˜åœ¨
        """
        return chunks_to_complete_response(self.chunks)
